---
title:  'Homework 3'
subtitle: 'INFO 523'
author:Madisyn Campbell
  affiliation: School of Information, University of Arizona, Tucson, AZ
- name: Instructor -  Cristian Román-Palacios
  affiliation: School of Information, University of Arizona, Tucson, AZ
tags: [R, RStudio, HW3]
output: html_document

---

---------------

### Objectives
This homework is divided into two parts. The written portion aims for a general description of one of the two datasets used in the hands-on portion of the assignment. In the hands-on component of the assignment, students will be asked to perform data manipulation operations using either (1) `base` `R` or the (2) `tidyverse`.

---------------

#### Additional resources relevant to this HW
- **R Markdown**: Please review the basic R Markdown cheat sheet in case you have any questions regarding formatting the HW: https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf.

- **R**: Please review the basic `R` cheat sheet in case you have any questions regarding the programming language: https://www.soa.org/globalassets/assets/Files/Edu/2018/exam-pa-base-r.pdf.

- **RStudio**: Additional cheat sheets written by RStudio to help with specific R packages: https://www.rstudio.com/resources/cheatsheets/


### Scores and grading policies

There is a very basic auto-grading system implemented at the end of the assignment. Use that workflow as a reference. **If you're confident that answer is correct but it's still marked as Incorrect by the autograder, please get in touch with the instructor.** Grades are **NOT exclusively based on your final answers**. I will be grading the overall structure and logic of your code. Feel free to use as many lines as you need to answer each of the questions. I also highly recommend and strongly encourage adding comments (`#`) to your code. Comments will certainly improve the reproducibility and readability of your submission. Commenting your code is also good coding practice. **Specifically for the course, you’ll get better feedback if the instructor is able to understand your code in detail.** 


### Submission:
Information on the deadline for this HW is posted in D2L.  Please get in touch with the instructor if you’re (i) having issues opening the assignment, (2) not understanding the questions or (3) having issues submitting your assignment. Note that late submissions are subject to a penalty (see late work policies in the Syllabus). By the deadline, you should turn in a a `RMD` file (this file) **AND** a rendered `HTML` (hint: knit your `rmd`; link: https://rmarkdown.rstudio.com/lesson-9.html). Answers to each question should be in the relevant block of code (see below). The instructor won't render your submission. **There's no need to rename your submission**. Make sure that you can correctly render your submission without errors before turning anything in. If a given block of code is causing issues and you didn’t get to fix it, please use `r eval=FALSE `the in the relevant block and add comments. **This assignment must be submitted through our GitHub Classroom before the deadline.**


### Time commitment
Please reach out if you’re taking more than ~18h to complete (1) this HW, (2) reading the book chapters, and (3) going over the lectures. I will be happy to provide accommodations if necessary. **Do not wait until the last minute to start working on this HW**. In most cases, working under pressure will certainly increase the time needed to answer each of these questions and the instructor might not be 100% available on Sundays to troubleshoot with you. Remember that you can sign up office hours with the instructor 3 times a week.


### Looking for help?
First, please go over the relevant readings for this week. Second, if you’re still struggling with any of the questions, do some independent research (e.g. stackoverflow is a wonderful resource). Don’t forget that your classmates will also be working on the same questions - reach out for help (check under the Discussion forum for folks looking to interact with other students in this class or start your own thread). Finally, the instructor will be happy to answer any questions during office hours. You can reach out to me by email (cromanpa94@arizona.edu) or simply schedule a 15 minute meeting through **Calendly** (https://calendly.com/cromanpa/15min). **Do not forget that the instructor holds office hours 3 times a week!!**


---------------
# Questions

### Conceptual

The conceptual questions outlined below are based on the following dataset: https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones.

Please answer the questions below in a few sentences (<300 words each). Save your answers as strings in the relevant objects (e.g. `q1`, `q2`, `q3`, `q4`). Use the structure outlined below in `q1.4.example`.

```{r exampleQ1-4}
q1.4.example <- "
[Paragraph 1]
[Paragraph 2]
[Paragraph n]
"
```

Please note that there's no need to provide extensive and detailed answers (i.e. <300 words). Just make sure that the main points that you're trying to make in relation to the question are clear.


#### Question 1

Briefly describe the dataset and explain how it has been used in previous analyses. 

```{r}
# BEGIN SOLUTION

# Loading data 

q1 <- "
The dataset is based on living activities from 30 particpants between 19-48 years old. Each participant is tracked on daily activities like walking, sitting, standing, laying, etc. while wearing a Samsung Galaxy smartphone on their waist. 

The dataset is useful for machine learning analysis. Recognizing trends in human activity can help predict the physical activity people are doing from the sensor readings. For example, the sensor readings can provide predicitions and analysis which helped create the smart watch. 
"

#Replace NULL with your answer (a string; see q1.example below)
# END SOLUTION
```


#### Question 2

List two questions that previous studies have asked using this dataset. Cite the relevant studies.

```{r}
# BEGIN SOLUTION
q2 <- "
How can activity sensor recognition models be used for individual users to improve accuracy and user experience over time?
Source: J. Wang et al. (2015) Personalized Human Activity Recognition Using Convolutional Neural Network

How does using data from various sensor readings improve individual activity recognition?
Source: R. Kwapisz (2011) Activity Recognition using Cell Phone Accelerometers
" 
#Replace NULL with your answer (a string)
# END SOLUTION
```


#### Question 3

There are two main folders in the dataset: training and testing. What are these files used for and what is their importance in Data Science in general?

```{r}
# BEGIN SOLUTION
q3 <- "
The training file is datasets used for training the machine learning models. The training data importance to data science, is because it helps build machine learning models to indentify future predictions and data patterns.

The test file is data after the machine learning model from the training data is evaluated. Test data is important to data science because it determines the accuracy and performance of the ML model as a whole.
" 
#Replace NULL with your answer (a string)
# END SOLUTION
```


#### Question 4

Choose one paper from the “Relevant Papers” section in the website. State the question that was examined in the study. Briefly outline the methods used in the paper (e.g. algorithms). Was there any data cleaning step? Explain. List the main conclusions of the study. Finally, include at least two suggestions that you think might help improve the quality of the paper. 

```{r}
# BEGIN SOLUTION
q4 <- "
Source: https://www.semanticscholar.org/paper/Enabling-Context-aware-Smart-Home-with-Semantic-Web-Zhang-Gu/e447fa4048cadac1133f03b7870d8bd12526db09

The main question is how can technology and sensor data be applied to smart homes to increase security and awareness?

Looking at the figures within the paper, some algorithms used were OWL/RDF graph and a Context Stack Model. 

Within the paper I didn't see any specific data cleaning processes, but typically for sensor readings and machine learnings noise reduction would be used to clean any inconclusive data. Integration could also be used to pull the different data sources and combine them for one analysis use.

In conclusion, the analysis looks at how semantic web tech can enhance awareness in smart homes. The semantic technology uses sensor data to advance applications in smart homes.

1. Having more explicit data cleaning steps.
2. Adding more data visualization aspects to show how the sensor data impacted the smart homes.
" 
#Replace NULL with your answer (a string)
# END SOLUTION
```


### Applied

Please answer the following questions using `R`. Feel free to use as many lines of code as you need. The final result should be stored in the object indicated under each question. I provide hints using either `base` `R` and or functions from the `tidyverse`. There's no need to follow these hints.


#### Question 5

We will be working with the following dataset: https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones. This dataset was also used in the conceptual section of this homework.  **Please DO NOT push the dataset to your `GitHub` repo** (e.g. use `gitignore`). **You will be penalized (-2 points) if your final submission to `Github` includes the original files of the analyzed dataset**. All the files in this homework are structured such that the dataset is not pushed to GitHub. 

Same as in the previous homeworks, answer the questions below by saving the final object in the relevant object. Feel free to also use as many steps or lines of code as you need to answer each of the questions. Please annotate your code.

I'll provide some help to download the dataset to your working directory:

```{r Do not modify this}
library(here)
library(downloader)
if(! "UCI HAR Dataset" %in% list.files("data")){
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip"
download(url, dest=here("data","dataset.zip"), mode="wb") 
unzip(here("data","dataset.zip"), exdir = here("data"))
}
```

Once you have run the code above, the downloaded folder will be located under `data`. **Note that you cannot commit files that are >100 mb**. Please check the size of the files that you're trying to commit **before** commiting+pushing to GitHub. Now, please focus on the questions below. All the instructions in the following questions are relative to the `UCI HAR Dataset` folder.

5.1.	Read the training set (`train/X_train.txt`) into `R` and name it `train`. Rename all the columns using the correct column name using the labels listed in the `train/features.txt` file. Make all column names unique. Save the resulting dataset as `q5.1`. Your answer should have `7352` rows and  `561` columns.

```{r}
# BEGIN SOLUTION

base::make.unique("UCI HAR Dataset")

# Read the training data
train <- read("UCI HAR Dataset", header = FALSE)

feature_names <- fread("./UCI HAR Dataset/features.txt", header = FALSE, col.names = c("index", "feature_name"))$feature_name

# Assign new column to data
colnames(train) <- make.names(feature_names, unique = TRUE)

# Save the resulting dataset
saveRDS(train, file = "q5.1.rds")


q5.1 <- q5.1.rds #Replace NULL with your final answer
# END SOLUTION
```

*Hint: (`Base` `R`): `make.unique()` *.


5.2.	Create a new column named `discrete` in `q5.1`. This column should include the content of `train/y_train.txt`. The resulting object should be saved in an object named `q5.2`. The resulting object should differ from `q5.1` by a single column.

```{r}
# BEGIN SOLUTION
q5.2 <- NULL #Replace NULL with your final answer
# END SOLUTION
```

*Hint: (tidyverse): mutate() and if_else() *


5.3.	Using `q5.2`, remove the `angle(X,gravityMean)` column. The resulting object should be named `q5.3`. The resulting object should have the same dimensions as `q5.1`, but not the exactly the same features.

```{r}
# BEGIN SOLUTION
q5.3 <- NULL #Replace NULL with your final answer
# END SOLUTION
```

*Hint: (base): grep() or grepl() *


5.4.	Next, reorder the columns in `q5.3` such that the columns with mean values (i.e. pattern *`mean`*) appear first in the dataset. Store the results in `q5.4`. 

```{r}
# BEGIN SOLUTION
q5.4 <- NULL #Replace NULL with your final answer
# END SOLUTION
```

*Hint: (tidyverse): select() and contains() *


5.5.	Using `q5.3`, select the first *4* rows of every column summarizing the *`angle`* between vectors (i.e. colum names including the string *`angle`*). Save the resulting dataset as `q5.5`. Your resulting dataset should have `4` rows and `6` columns.

```{r}
# BEGIN SOLUTION
q5.5 <- NULL #Replace NULL with your final answer
# END SOLUTION
```

*Hint: (base): grep() or grepl() *


5.6.	Using `q5.3`, remove all the rows with *at least* one observation falling outside of quantiles `0.025` and `0.975` of *at least one* column in the dataset. Note that these observations are referred as "extreme" in Question 5.7. Store the resulting object in `q5.6`. Do not use the full dataset for this question - please sample *ONLY* the first `1000` rows and `20` columns. Your answer should have `644` rows and  `20` columns.

*One option (using `base` `r`): Estimate the `0.025` and `0.975` quantiles for each column in `q5.3`. Save the resulting data.frame to the environment (e.g. quantileDataset object). Next, for a given row in `q5.3`, sample the a column and test if the sampled value in `q5.3` (i.e. [row n,column n]) is within the interval estimated for that  column in `quantileDataset`. If the value in `q5.3` is outside of the interval (`quantileDataset`), replace the cell with an `NA`. If the value in `q5.3` is not outside of the interval, keep the original value for that cell in `q5.3`. Do the same for each observation (i.e. column) in each row. Finally, drop all the `NAs` in your dataset. Please do NOT overwrite your answer for `q5.3`*

```{r}
# BEGIN SOLUTION
q5.6 <- NULL #Replace NULL with your final answer
# END SOLUTION
```

*Hint: (base): for(), if(), na.omit() *

*Hint: (tidyverse): mutate(), across(), if_else(), na.omit()*


#### Question 6 (extra credit, 1 point each)

Read in the `data/population-figures-by-country-csv.csv` file that is included in your repo into `R` and simply call it `data`. This dataset contains population information on almost every country between `1960` and `2016`. Make sure that you remove line `257` (i.e. country `World`) before you get started with the questions below.

6.1.	First, rename the second column of `data` to `code`. Now, reshape `data` from wide to long format. Sort the resulting dataset by country first and then by year. Next, for each country, keep only the oldest and most recent years with non-missing values for population sizes. Based these two years and corresponding population values, calculate the following three columns: (1) `Diff_year` (difference in time between years), (2) `Diff_growth` (difference in population size between years), and (3) `Rate_percent` = ((`Diff_growth` / `Diff_year`)/`oldest_year`). The resulting dataset should only include only the following columns: `Country`, `code`, `Diff_year`, `Diff_growth`, and `Rate_percent.` Save the resulting object in `q6.1`. Feel free to use as many lines of code as needed.

```{r}
# BEGIN SOLUTION
q6.1 <- NULL #Replace NULL with your final answer
# END SOLUTION
```


6.2.	Split `q6.1` into a continent-based list. Save the result in `q6.2`. Use only the countries that overlap with the following dataset `data/country-and-continent-codes-list.csv`. Please provide two ideas on how this new list could be used to perform calculations with functions within the `apply` family.

```{r}
# BEGIN SOLUTION
q6.2 <- NULL #Replace NULL with your final answer
# END SOLUTION
```



## Some quick feedback

**Do NOT modify this section.** Note that the auto-grader tests whether objects in questions 1-4 include any string, and examine if dimensions (i.e. number of columns, rows, or length) of the objects in questions 5 and 6 match a reference.

```{r echo=FALSE, message=FALSE, warning=FALSE, comment=NA}
##Please make sure the following packages are installed

library(digest); library(here); library(rmarkdown); 
load(here("tests", "ref_WS.RData"))
rm(list=setdiff(ls(), c("q1", "q2", "q3", "q4", "q5.1", "q5.2",
                       "q5.3", "q5.4", "q5.5", "q5.6", 
                       "q6.1", "q6.2"))) #Prevent saving extra objects...
save.image(here("tests", "answers_WS.RData"))
.grade(submission = list(q1, q2, q3, q4, q5.1, q5.2,
                       q5.3, q5.4, q5.5, q5.6, 
                       q6.1, q6.2
                       )
       )

cat("Last updated on:", format(Sys.time(),usetz = TRUE))
```

```{r echo= FALSE, message=FALSE, warning=FALSE, include=FALSE, results='hide'}
render("README.Rmd", quiet = T)
```


